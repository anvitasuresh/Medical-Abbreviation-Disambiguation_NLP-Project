{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024d9490-e244-4dbc-a348-d74436117118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_synthetic, load_real\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_data, extract_context\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.data_loader import load_synthetic, load_real\n",
    "from src.feature_extraction import process_data, extract_context\n",
    "from src.models import MultinomialNB\n",
    "from src.tfidf import compute_tfidf\n",
    "from src.evaluation import calculate_metrics, confusion_matrix\n",
    "\n",
    "# Load and process data\n",
    "print(\"Loading real data...\")\n",
    "real_df = load_real()\n",
    "X, y, vocab = process_data(real_df, min_freq=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, np.arange(len(X)), test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {(y_pred == y_test).mean():.4f}\")\n",
    "\n",
    "# 1. CONFUSION MATRIX HEATMAP\n",
    "classes = np.unique(y_train)\n",
    "cm = confusion_matrix(y_test, y_pred, classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[c[:15] for c in classes], \n",
    "            yticklabels=[c[:15] for c in classes])\n",
    "plt.title('Confusion Matrix - Real Data')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. PER-ABBREVIATION PERFORMANCE\n",
    "abbrev_results = []\n",
    "for abbrev in ['CC', 'CP', 'SA']:\n",
    "    mask = real_df.loc[idx_test, 'abbreviation'].values == abbrev\n",
    "    abbrev_true = y_test[mask]\n",
    "    abbrev_pred = y_pred[mask]\n",
    "    accuracy = (abbrev_true == abbrev_pred).mean()\n",
    "    abbrev_results.append({'Abbreviation': abbrev, 'Accuracy': accuracy, 'Count': mask.sum()})\n",
    "\n",
    "abbrev_df = pd.DataFrame(abbrev_results)\n",
    "print(\"\\nPer-Abbreviation Performance:\")\n",
    "display(abbrev_df)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(abbrev_df['Abbreviation'], abbrev_df['Accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Abbreviation')\n",
    "plt.title('Accuracy by Abbreviation')\n",
    "plt.ylim(0, 1)\n",
    "plt.axhline(y=0.788, color='r', linestyle='--', label='Overall')\n",
    "plt.legend()\n",
    "plt.savefig('results/per_abbrev_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. CLASS-WISE METRICS TABLE\n",
    "metrics = calculate_metrics(y_test, y_pred, classes)\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df = metrics_df.round(4)\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "display(metrics_df.style.background_gradient(cmap='RdYlGn', vmin=0.5, vmax=1.0))\n",
    "\n",
    "# 4. MOST CONFUSED PAIRS\n",
    "confusion_pairs = []\n",
    "for i, true_class in enumerate(classes):\n",
    "    for j, pred_class in enumerate(classes):\n",
    "        if i != j and cm.iloc[i, j] > 0:\n",
    "            confusion_pairs.append({\n",
    "                'True': true_class,\n",
    "                'Predicted': pred_class,\n",
    "                'Count': cm.iloc[i, j]\n",
    "            })\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_pairs).sort_values('Count', ascending=False).head(10)\n",
    "print(\"\\nMost Confused Pairs:\")\n",
    "display(confusion_df)\n",
    "\n",
    "# 5. FAILURE EXAMPLES\n",
    "failure_mask = y_pred != y_test\n",
    "failure_indices = idx_test[failure_mask]\n",
    "sample_failures = np.random.choice(failure_indices, size=15, replace=False)\n",
    "\n",
    "failure_examples = []\n",
    "for idx in sample_failures:\n",
    "    row = real_df.loc[idx]\n",
    "    pred_idx = np.where(idx_test == idx)[0][0]\n",
    "    context_words = extract_context(row)\n",
    "    failure_examples.append({\n",
    "        'Abbrev': row['abbreviation'],\n",
    "        'True': row['label'],\n",
    "        'Predicted': y_pred[pred_idx],\n",
    "        'Context': ' '.join(context_words[:20]) + '...'\n",
    "    })\n",
    "\n",
    "failure_df = pd.DataFrame(failure_examples)\n",
    "print(\"\\nFailure Examples:\")\n",
    "display(failure_df)\n",
    "\n",
    "# 6. COMPARISON: SYNTHETIC VS REAL\n",
    "comparison = pd.DataFrame({\n",
    "    'Dataset': ['Synthetic', 'Real (Raw)', 'Real (TF-IDF)'],\n",
    "    'Accuracy': [0.9963, 0.7880, 0.7845]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(comparison['Dataset'], comparison['Accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig('results/comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12209ace-df50-45ef-8f62-346fe51282fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
